../ryla/main.py :

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import firebase_admin
from firebase_admin import credentials, db
from models import UserInput, ProcessedResponse
from src.assistant import MultilingualAssistant
from datetime import datetime

from config import get_settings
settings = get_settings()

import logging
from datetime import datetime

# # Initialize Firebase
# cred = credentials.Certificate(settings.firebase_credentials_path)
# firebase_admin.initialize_app(cred, {
#     'databaseURL': settings.firebase_database_url
# })

# Initialize Firebase only if not already initialized

def initialize_firebase():
    try:
        firebase_admin.get_app()
    except ValueError:
        cred = credentials.Certificate(settings.firebase_credentials_path)
        firebase_admin.initialize_app(cred, {
            'databaseURL': settings.firebase_database_url
        })

# Initialize FastAPI
app = FastAPI(
    title="Multilingual Assistant API",
    description="API for processing multilingual text with grammar correction and response generation",
    version="1.0.0"
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:8081"],  # Update this with your React Native app's URL in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize the assistant
assistant = MultilingualAssistant()

@app.on_event("startup")
async def startup_event():
    initialize_firebase()
    logging.info("Application started, Firebase initialized")

# @app.post("/process_text", response_model=ProcessedResponse)
# async def process_text(user_input: UserInput):
#     try:
#         user_ref = db.reference(f'users/uUaWUUcfiTcjPlBKYmrZu4E5yyJ2/model_data')
#         user_data = user_ref.get()
        
#         if not user_data:
#             raise HTTPException(status_code=404, detail="User preferences not found")
        
#         result = await assistant.process_input(
#             text=user_input.text,
#             language=user_data['lang_to_learn'],
#             proficiency=user_data['proficiency_level'],
#             target="grammar_correction" #user_data['target_use']
#         )
        
#         return ProcessedResponse(**result)
        
#     except Exception as e:
#         logging.error(f"Error processing text: {e}")
#         raise HTTPException(status_code=500, detail=str(e))

@app.post("/process_text", response_model=ProcessedResponse)
async def process_text(user_input: UserInput):
    try:
        # For testing without Firebase, use hardcoded values
        user_data = {
            'lang_to_learn': 'fr',  # or 'fr'
            'proficiency_level': 'intermediate',  # or 'beginner' or 'expert'
        }
        
        result = await assistant.process_input(
            text=user_input.text,
            language=user_data['lang_to_learn'],
            proficiency=user_data['proficiency_level'],
            target="grammar_correction"
        )
        
        return ProcessedResponse(**result)
        
    except Exception as e:
        logging.error(f"Error processing text: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": str(datetime.now()),
        "environment": settings.environment
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host=settings.host,
        port=settings.port,
        reload=settings.environment == "development"
    )


../ryla/models.py :

from pydantic import BaseModel
from typing import Optional, Dict, Any

class UserInput(BaseModel):
    text: str
    user_id: str

class UserPreferences(BaseModel):
    lang_to_learn: str
    proficiency_level: str
    target_use: str

class ProcessedResponse(BaseModel):
    original_text: str
    corrected_text: str
    response: str
    metadata: Optional[Dict[str, Any]] = None


../ryla/config.py :

from pydantic_settings import BaseSettings
from functools import lru_cache

class Settings(BaseSettings):
    environment: str = "development"
    host: str = "localhost"
    port: int = 8000
    firebase_credentials_path: str = "./firebase_credentials.json"
    firebase_database_url: str = "your_firebase_database_url"
    model_cache_dir: str = "./model_cache"

    class Config:
        env_file = ".env"

@lru_cache()
def get_settings():
    return Settings()

settings = get_settings()



../ryla/.env :

FIREBASE_CREDENTIALS_PATH="..\Ryla\Firebase_connection.json"
FIREBASE_DATABASE_URL="link"
MODEL_CACHE_DIR="./model_cache"
PORT=8000
HOST="0.0.0.0"
ENVIRONMENT="development"


../ryla/src/assistant.py :

# import torch
# from transformers import (
#     AutoTokenizer,
#     BlenderbotTokenizer,
#     BlenderbotForConditionalGeneration,
#     AutoModelForCausalLM,
#     AutoModelForSeq2SeqLM
# )

# from typing import Optional, Dict, Any
# import os
# os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

# from datetime import datetime

# from config import get_settings
# settings = get_settings()

# import logging

# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)

# class MultilingualAssistant:
#     def __init__(self):
#         self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#         logger.info(f"Using device: {self.device}")
        
#         self.language_configs = {
#             'fr': {
#                 'grammar_model': "PoloHuggingface/French_grammar_error_corrector",
#                 'chat_model': "microsoft/DialoGPT-medium",
#                 'tokenizer_class': AutoTokenizer,
#                 'model_class': AutoModelForCausalLM
#             },
#             'en': {
#                 'grammar_model': "grammarly/coedit-large",
#                 'chat_model': "facebook/blenderbot-400M-distill",
#                 'tokenizer_class': BlenderbotTokenizer,
#                 'model_class': BlenderbotForConditionalGeneration
#             }
#         }
        
#         self.target_uses = {
#             'fr': {
#                 'grammar_correction': {'prompt': "Corriger la grammaire: ", 'weight': 1.0},
#                 'text_coherent': {'prompt': "Rendre ce texte plus cohérent: ", 'weight': 0.8},
#                 'easier_understanding': {'prompt': "Simplifier ce texte: ", 'weight': 0.6},
#                 'paraphrasing': {'prompt': "Paraphraser ce texte: ", 'weight': 0.7},
#                 'formal_tone': {'prompt': "Rendre le texte plus formel: ", 'weight': 0.9},
#                 'neutral_tone': {'prompt': "Convertir le texte en ton neutre: ", 'weight': 0.8}
#             },
#             'en': {
#                 'grammar_correction': {'prompt': "Correct the grammar: ", 'weight': 1.0},
#                 'text_coherent': {'prompt': "Make this text more coherent: ", 'weight': 0.8},
#                 'easier_understanding': {'prompt': "Simplify this text: ", 'weight': 0.6},
#                 'paraphrasing': {'prompt': "Paraphrase this text: ", 'weight': 0.7},
#                 'formal_tone': {'prompt': "Make the text more formal: ", 'weight': 0.9},
#                 'neutral_tone': {'prompt': "Convert text to a neutral tone: ", 'weight': 0.8}
#             }
#         }
        
#         self.model_configs = {
#             'beginner': {
#                 'complexity': 0.1,
#                 'max_length': 50,
#                 'context_prompts': {
#                     'fr': ["Répondre de manière très basique: "],
#                     'en': ["Reply in a very basic manner: "]
#                 }
#             },
#             'intermediate': {
#                 'complexity': 0.6,
#                 'max_length': 100,
#                 'context_prompts': {
#                     'fr': ["Répondre en utilisant une variété de mots: "],
#                     'en': ["Reply using variety of words: "]
#                 }
#             },
#             'expert': {
#                 'complexity': 1.0,
#                 'max_length': 150,
#                 'context_prompts': {
#                     'fr': ["Répondre en utilisant des phrases complexes: "],
#                     'en': ["Reply using complex sentences: "]
#                 }
#             }
#         }
        
#         self.models = {'fr': {}, 'en': {}}
#         os.makedirs(settings.model_cache_dir, exist_ok=True)

#     async def load_language_models(self, language: str):
#         if language not in self.models or not self.models[language]:
#             config = self.language_configs[language]
#             try:
#                 logger.info(f"Loading models for {language}")
                
#                 # Load models with cache
#                 cache_dir = os.path.join(settings.model_cache_dir, language)
                
#                 self.models[language] = {
#                     'grammar_tokenizer': AutoTokenizer.from_pretrained(
#                         config['grammar_model'],
#                         cache_dir=cache_dir
#                     ),
#                     'grammar_model': AutoModelForSeq2SeqLM.from_pretrained(
#                         config['grammar_model'],
#                         cache_dir=cache_dir
#                     ).to(self.device),
#                     'chat_tokenizer': config['tokenizer_class'].from_pretrained(
#                         config['chat_model'],
#                         cache_dir=cache_dir
#                     ),
#                     'chat_model': config['model_class'].from_pretrained(
#                         config['chat_model'],
#                         cache_dir=cache_dir
#                     ).to(self.device)
#                 }
                
#                 logger.info(f"Successfully loaded models for {language}")
#             except Exception as e:
#                 logger.error(f"Error loading models for {language}: {e}")
#                 raise

#     async def process_input(self, text: str, language: str, proficiency: str, target: str) -> Dict[str, Any]:
#         await self.load_language_models(language)
        
#         # Process in parallel using asyncio
#         import asyncio
#         correction_task = asyncio.create_task(self.correct_grammar(text, language, target))
        
#         correction = await correction_task
#         response = await self.generate_response(correction or text, language, proficiency)
        
#         return {
#             "original_text": text,
#             "corrected_text": correction if correction else text,
#             "response": response,
#             "metadata": {
#                 "language": language,
#                 "proficiency": proficiency,
#                 "target": target,
#                 "processed_timestamp": str(datetime.now())
#             }
#         }

#     async def correct_grammar(self, input_text: str, language: str, target: str) -> Optional[str]:
#         if len(input_text.split()) <= 1:
#             return None

#         try:
#             target_config = self.target_uses[language][target]
#             grammar_input = f"{target_config['prompt']}{input_text}"
            
#             models = self.models[language]
#             input_ids = models['grammar_tokenizer'](
#                 grammar_input,
#                 return_tensors="pt",
#                 padding=True,
#                 truncation=True,
#                 max_length=512
#             ).input_ids.to(self.device)
            
#             with torch.no_grad():
#                 outputs = models['grammar_model'].generate(
#                     input_ids,
#                     max_length=min(512, len(input_text.split()) * 2),
#                     num_beams=5,
#                     do_sample=True,
#                     temperature=target_config['weight'],
#                     top_p=0.9,
#                     repetition_penalty=1.1,
#                     early_stopping=True
#                 )
            
#             corrected = models['grammar_tokenizer'].decode(outputs[0], skip_special_tokens=True)
#             return corrected if corrected.lower() != input_text.lower() else None

#         except Exception as e:
#             logger.error(f"Grammar correction error: {e}")
#             return None

#     async def generate_response(self, input_text: str, language: str, proficiency: str) -> str:
#         try:
#             config = self.model_configs[proficiency]
#             import random
#             context = random.choice(config['context_prompts'][language])
#             modified_input = f"{context}{input_text}"
            
#             models = self.models[language]
#             input_data = models['chat_tokenizer'](
#                 modified_input,
#                 return_tensors="pt",
#                 padding=True,
#                 truncation=True,
#                 max_length=512
#             ).to(self.device)
            
#             with torch.no_grad():
#                 output_ids = models['chat_model'].generate(
#                     **input_data,
#                     max_length=config['max_length'],
#                     num_beams=4,
#                     do_sample=True,
#                     temperature=config['complexity'],
#                     top_p=0.9,
#                     repetition_penalty=1.2,
#                     early_stopping=True
#                 )
            
#             return models['chat_tokenizer'].decode(output_ids[0], skip_special_tokens=True)

#         except Exception as e:
#             logger.error(f"Response generation error: {e}")
#             return "I'm having trouble understanding. Could you rephrase that?"




# src/assistant.py
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from typing import Dict, Any, Optional
import os
import asyncio
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MultilingualAssistant:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"Using device: {self.device}")
        
        self.language_configs = {
            'fr': {
                'grammar_model': "PoloHuggingface/French_grammar_error_corrector",
                'response_model': "microsoft/DialoGPT-medium",
            },
            'en': {
                'grammar_model': "grammarly/coedit-large",
                'response_model': "facebook/blenderbot-400M-distill"
            }
        }
        
        self.models = {'fr': {}, 'en': {}}
        self.is_loading = {'fr': False, 'en': False}
        self.load_lock = asyncio.Lock()
        
        # Pre-load English models at initialization
        asyncio.create_task(self.load_language_models('fr'))

    async def load_language_models(self, language: str):
        async with self.load_lock:
            if language not in self.models or not self.models[language]:
                if self.is_loading[language]:
                    while self.is_loading[language]:
                        await asyncio.sleep(0.1)
                    return
                
                self.is_loading[language] = True
                config = self.language_configs[language]
                
                try:
                    logger.info(f"Loading models for {language}")
                    
                    # Load models with pipeline for optimization
                    self.models[language] = {
                        'grammar': pipeline(
                            "text2text-generation",
                            model=config['grammar_model'],
                            device=0 if torch.cuda.is_available() else -1
                        ),
                        'response': pipeline(
                            "text2text-generation",
                            model=config['response_model'],
                            device=0 if torch.cuda.is_available() else -1
                        )
                    }
                    
                    logger.info(f"Successfully loaded models for {language}")
                except Exception as e:
                    logger.error(f"Error loading models for {language}: {e}")
                    raise
                finally:
                    self.is_loading[language] = False

    async def process_input(self, text: str, language: str, proficiency: str, target: str) -> Dict[str, Any]:
        # Start loading models if not already loaded
        load_task = asyncio.create_task(self.load_language_models(language))
        await load_task

        if not text.strip():
            return {
                "original_text": text,
                "corrected_text": text,
                "response": "Please provide some text to process.",
                "metadata": {"language": language, "processed_timestamp": str(datetime.now())}
            }

        # Process text
        try:
            correction = await self.correct_grammar(text, language)
            response = await self.generate_response(correction or text, language, proficiency)
            
            return {
                "original_text": text,
                "corrected_text": correction if correction else text,
                "response": response,
                "metadata": {
                    "language": language,
                    "proficiency": proficiency,
                    "processed_timestamp": str(datetime.now())
                }
            }
        except Exception as e:
            logger.error(f"Processing error: {e}")
            return {
                "original_text": text,
                "corrected_text": text,
                "response": "An error occurred while processing your text.",
                "metadata": {"error": str(e)}
            }

    async def correct_grammar(self, input_text: str, language: str) -> Optional[str]:
        try:
            if language == 'fr':
                prompt = f"Corriger la grammaire: {input_text}"
            else:
                prompt = f"Correct this text: {input_text}"
            
            result = self.models[language]['grammar'](
                prompt,
                max_length=100,
                num_beams=5,
                do_sample=True,
                temperature=0.7
            )
            
            return result[0]['generated_text']
        except Exception as e:
            logger.error(f"Grammar correction error: {e}")
            return None

    async def generate_response(self, input_text: str, language: str, proficiency: str) -> str:
        try:
            if language == 'fr':
                prompt = f"Répondre à: {input_text}"
            else:
                prompt = f"Respond to: {input_text}"
            
            result = self.models[language]['response'](
                prompt,
                max_length=100,
                num_beams=4,
                do_sample=True,
                temperature=0.8
            )
            
            return result[0]['generated_text']
        except Exception as e:
            logger.error(f"Response generation error: {e}")
            return "I'm having trouble understanding. Could you rephrase that?"